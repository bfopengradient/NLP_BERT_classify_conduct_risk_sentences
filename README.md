  
Testing Bert(large) on Conduct Risk related sentences. 

Testing the large BERT model with just a small amount of sentences. 70 sentences were shown to the BERT model, 27 were of interest. The model was then tested on several sentences to see how it performed on the classification task. 

Versus other embedding pre-trained vectors that are available and not domain specific the results were encouraging in this test. 

The idea behind the test was to check how few hard coded sentences needed to be shown to the model during training. One of the anticipated benefits of working with a large pre-trained model such as BERT is the ability to cut down on the amount of hard coding required to catch information of interest and cut back on false positives at the same time. 

Apr/2019
